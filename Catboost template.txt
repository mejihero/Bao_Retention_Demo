#######################################################
#########  this is a template for CatBoost  ###########


from catboost import CatBoostClassifier


#column index for categorical features
cat_features = [0,1,2,3,4,5]


# hyperparameter grid search
param_grid = {'depth': [4,6,8,10],
             'learning_rate': [0.01, 0.03, 0.1]}


##################################
# more hyper parameter grid search
##################################

param_grid = {'depth': [4,6,8,10],
             'learning_rate': [0.01, 0.03, 0.1],
             'l2_leaf_reg': [1, 3, 5, 7, 9],
             'iterations': [100, 200, 300, 400, 500, 600]}





# fit the initial catboost model
mod = CatBoostClassifier(#custom_loss = ['Accuracy'],
                        custom_metric = ['AUC'],
                        random_seed = 123,
                         loss_function = 'Logloss',
                        logging_level = 'Verbose')




# grid search for the catboost model
from sklearn.model_selection import GridSearchCV

grid = GridSearchCV(estimator = mod,
                 param_grid = param_grid,
                 scoring = 'roc_auc',
                 cv = 5,
                 verbose = 1)


#specify the categorical feature
grid.fit(X_train, y_train,
        cat_features = cat_features)





#re-fit catboost model with the optimal hyperparameter 
classifier = CatBoostClassifier(iterations = 500,
                               depth = grid.best_params_['depth'],
                               learning_rate = grid.best_params_['learning_rate'],
                                 custom_metric = ['AUC'],
                        random_seed = 123,
                         loss_function = 'Logloss',
                        logging_level = 'Verbose')





##########################################################
#refit models with more optimal hyper-parameters
##########################################################

classifier2 = CatBoostClassifier(depth = grid.best_params_['depth'],
                               learning_rate = grid.best_params_['learning_rate'],
                                l2_leaf_reg = grid.best_params_['l2_leaf_reg'],
                                iterations = grid.best_params_['iterations'],
                                 custom_metric = ['AUC'],
                        random_seed = 123,
                         loss_function = 'Logloss',
                        logging_level = 'Verbose'
                                )





#fit catboost model with the optimal hyperparameter 
classifier.fit(X_train, y_train,
              cat_features = cat_features)



#prediction based on the test dataset

y_pred = classifier.predict(X_test)

y_pred_prob = classifier.predict_proba(X_test)



#model performance

from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import roc_auc_score

import scikitplot as skplt


# confusion matrix
skplt.metrics.plot_confusion_matrix(y_test, y_pred)




#report matrix
print(classification_report(y_test, y_pred))




#roc curve
skplt.metrics.plot_roc(y_test, y_pred_prob)



#precision recall curve
skplt.metrics.plot_precision_recall_curve(y_test, y_pred_prob)



# feature importance
feature_imp = pd.DataFrame(data = classifier.feature_importances_, index = X_test.columns.values, columns = ['values'])
feature_imp.sort_values(['values'], ascending = False, inplace = True)
feature_imp.reset_index(level = 0, inplace = True)

feature_imp




#feature importance plot
import seaborn as sns
import matplotlib.pyplot as plt

%matplotlib inline


ax = sns.barplot(x = 'values', y = 'index', data = feature_imp, palette = 'deep')
ax.set_title('Variables Imporatnce Plot of Catboost')






























