{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style = 'white', font_scale = 0.9)\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import scikitplot as skplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_mod(X_train, y_train, X_test, y_test, sampName):\n",
    "    \n",
    "    ## print the shape of imported dataset\n",
    "    print('X_train shape: ', X_train.shape)\n",
    "    print('y_train shape: ', y_train.shape)\n",
    "    print('X_test shape: ', X_test.shape)\n",
    "    print('y_test shape: ', y_test.shape)\n",
    "    \n",
    "    ## grid search for the optimal hyperparameters\n",
    "    model = XGBClassifier(random_state = 123, objective = 'binary:logistic', nthread = 4, seed = 123) #scale_pos_weight\n",
    "    \n",
    "    param_grid = {'n_estimators': [100,200,300,400,500], \n",
    "                  'learning_rate': [0.01, 0.1, 0.5],\n",
    "                 'max_depth': [15, 20, 25, 30], \n",
    "                  'min_child_weight': [1,2,3], \n",
    "                  'gamma': [0.1,0.2],\n",
    "                 'colsample_bytree': [0.6,0.7], \n",
    "                  'subsample': [0.6, 0.7], \n",
    "                  'reg_alpha': [1e-5,1e-2,0.1,1,100]}\n",
    "    \n",
    "    grid = GridSearchCV(estimator = model, \n",
    "                        param_grid = param_grid, \n",
    "                        scoring = 'roc_auc', \n",
    "                        n_jobs = -1, \n",
    "                        verbose = 1)\n",
    "    \n",
    "    grid.fit(X_train, y_train)\n",
    "    \n",
    "    print(grid.best_score_)\n",
    "    print(grid.best_params_)\n",
    "    \n",
    "    ## fit the model with the optimal hyperparameters\n",
    "    classifier = XGBClassifier(random_state = 123,\n",
    "                               n_estimators = grid.best_params_['n_estimators'],\n",
    "                               learning_rate = grid.best_params_['learning_rate'],\n",
    "                               max_depth = grid.best_params_['max_depth'],\n",
    "                               min_child_weight = grid.best_params_['min_child_weight'],\n",
    "                               gamma = grid.best_params_['gamma'],\n",
    "                               colsample_bytree = grid.best_params_['colsample_bytree'],\n",
    "                               subsample = grid.best_params_['subsample'],\n",
    "                               reg_alpha = grid.best_params_['reg_alpha'])\n",
    "    \n",
    "    classifier.fit(X_train, y_train)\n",
    "    \n",
    "    ## pickle the fitted model\n",
    "    joblib.dump(classifier, 'xgboost_'+sampName+'.pkl')\n",
    "    \n",
    "    ## pickle the predicted responses\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    y_pred_prob = classifier.predict_proba(X_test)\n",
    "    \n",
    "    joblib.dump(y_pred, 'y_pred_'+sampName+'.pkl')\n",
    "    joblib.dump(y_pred_prob, 'y_pred_prob_'+sampName+'.pkl')\n",
    "    \n",
    "    # printout metrics for model performance\n",
    "    print('Accuracy score: ', accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    print('Confusion matrix:\\n', confusion_matrix(y_test, y_pred))\n",
    "    skplt.metrics.plot_confusion_matrix(y_test, y_pred)\n",
    "    plt.savefig('confusion_matrix_xgboost.png')\n",
    "    \n",
    "    skplt.metrics.plot_confusion_matrix(y_test, y_pred, normalize = True)\n",
    "    plt.savefig('confusion_matrix_xgboost_norm.png')\n",
    "    \n",
    "    \n",
    "    print('Classification report:\\n', classification_report(y_test, y_pred))\n",
    "    \n",
    "    print('ROC_AUC_score: ', roc_auc_score(y_test, y_pred))\n",
    "    skplt.metrics.plot_roc(y_test, y_pred_prob)\n",
    "    plt.savefig('roc_xgboost.png')\n",
    "    \n",
    "    skplt.metrics.plot_precision_recall_curve(y_test, y_pred_prob)\n",
    "    plt.savefig('pr_xgboost.png')\n",
    "    \n",
    "    # printout feature importances\n",
    "    feature_import = pd.DataFrame(data = classifier.feature_importances_, index = X_test.columns.values, columns = ['values'])\n",
    "    feature_import.sort_values(['values'], ascending = False, inplace = True)\n",
    "    feature_import.reset_index(level = 0, inplace = True)\n",
    "    print(feature_import)\n",
    "    \n",
    "    # feature importances plot\n",
    "    plt.figure(figsize = (35,20))\n",
    "    sns.set(style = 'white', font_scale = 2)\n",
    "    \n",
    "    ax = sns.barplot(x = 'values', y = 'index', data = feature_import, palette = 'deep')\n",
    "    ax.set_title('Variables Imporatnce Plot of XGBoost')\n",
    "    plt.savefig('importance_xgboost.png')\n",
    "    \n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LUY1\\Desktop\\customer churn data\\data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('C:/Users/LUY1/Desktop/customer churn data/data')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle('X_train.pkl')\n",
    "X_test = pd.read_pickle('X_test.pkl')\n",
    "\n",
    "y_train = pd.read_pickle('y_train.pkl')\n",
    "y_test = pd.read_pickle('y_test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LUY1\\Desktop\\customer churn data\\models\n"
     ]
    }
   ],
   "source": [
    "os.chdir('C:/Users/LUY1/Desktop/customer churn data/models')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (419420, 19)\n",
      "y_train shape:  (419420,)\n",
      "X_test shape:  (104855, 19)\n",
      "y_test shape:  (104855,)\n",
      "Fitting 3 folds for each of 7200 candidates, totalling 21600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 42.1min\n"
     ]
    }
   ],
   "source": [
    "classifier_mod(X_train, y_train, X_test, y_test, 'nosamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
